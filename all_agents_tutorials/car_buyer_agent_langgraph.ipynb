{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smart Car Buyer AI Agent\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook presents an intelligent car-buying assistant using LangGraph and an LLM model. The system assists users in defining their car-buying needs, refines search filters, and retrieves relevant listings, providing a streamlined buying experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langgraph\n",
    "%pip install langchain\n",
    "%pip install langchain-openai\n",
    "%pip install importnb\n",
    "%pip install python-dotenv\n",
    "%pip install patchright\n",
    "%pip install lxml\n",
    "%pip install playwright"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from typing import TypedDict, Dict, List, Any\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from IPython.display import display, Image\n",
    "from langchain_core.runnables.graph import MermaidDrawMethod\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "from langsmith import Client\n",
    "from langsmith import traceable\n",
    "from langsmith.wrappers import wrap_openai\n",
    "import openai\n",
    "import asyncio\n",
    "from importnb import Notebook\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# This import is required only for jupyter notebooks, since they have their own eventloop\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "with Notebook():\n",
    "    from scrapers.autotrader import AutotraderInterface, WebsiteInterface\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!playwright install\n",
    "!patchright install chromium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"car_buyer_agent\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv('LANGCHAIN_API_KEY', \"\")\n",
    "\n",
    "# Initialize the language model\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LangSmith client\n",
    "langsmith_client = Client()\n",
    "\n",
    "openai_client = wrap_openai(openai.Client())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    \"\"\"Represents the state of the car-buying process.\"\"\"\n",
    "    user_needs: str\n",
    "    web_interfaces: List[WebsiteInterface]\n",
    "    listings: List[Dict[str, str]]\n",
    "    selected_listing: Dict[str, str]\n",
    "    additional_info: Dict[str, str]\n",
    "    user_input: str\n",
    "    next_node: str\n",
    "    messages: List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_user_needs(state: State) -> State:\n",
    "    \"\"\"Ask user initial questions to define their needs for the car.\"\"\"\n",
    "    \n",
    "    messages = state.get(\"messages\", [])\n",
    "    \n",
    "    system_message = \"\"\n",
    "    \n",
    "    if len(messages) == 0:\n",
    "        system_message += \"You are a car buying assistant. Your goal is to help the user find a car that meets their needs. Start by introducing yourself and asking about their requirements, such as intended usage (e.g., commuting, family trips), budget, size preferences, and any specific constraints or features they value. Use their responses to guide them toward the best options.\"\n",
    "    else:\n",
    "        system_message += \"Ask the user for any additional information that can help narrow down the search. If he asked any questions before, answer them before asking for more information.\"\n",
    "        \n",
    "        \n",
    "    # Check if we already have some user needs to preserve\n",
    "    existing_needs = state.get(\"user_needs\", \"\")\n",
    "    if existing_needs:\n",
    "        system_message += f\" Here's what we know about the needs of the user so far:\\n\\n{existing_needs}\"\n",
    "    \n",
    "    messages.append(SystemMessage(system_message))\n",
    "    \n",
    "    # Get message from the LLM\n",
    "    response = llm.invoke(messages).content\n",
    "    \n",
    "    print(response)\n",
    "    \n",
    "    messages += [AIMessage(response)]\n",
    "    \n",
    "    messages += [HumanMessage(input(response))]\n",
    "    \n",
    "    messages += [\n",
    "        SystemMessage(\n",
    "            \"Summarize the user's car-buying needs in one clear and concise sentence based on their input and any prior knowledge.\\n\"\n",
    "            \"Provide the summary as the first line of your response.\\n\"\n",
    "            \"Then, skip two lines and indicate the next step by writing either 'ask_user_needs' or 'build_filters':\\n\"\n",
    "            \"- Use 'ask_user_needs' if you need more information or if the user asked a question.\\n\"\n",
    "            \"- Use 'build_filters' if you have enough details to search for cars online.\\n\"\n",
    "            \"Do not write anything else. Reserve questions and explanations for later.\"\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    response = llm.invoke(messages).content\n",
    "    \n",
    "    messages += [AIMessage(response)]\n",
    "\n",
    "    state[\"user_needs\"] = response.split(\"\\n\")[0].strip()\n",
    "    \n",
    "    print(\"\\nI have summarized your car-buying needs as follows:\")\n",
    "    print(state[\"user_needs\"])\n",
    "    \n",
    "    if len(response.split(\"\\n\")) > 1:\n",
    "        state[\"next_node\"] = response.split(\"\\n\")[-1].strip()\n",
    "    else:\n",
    "        state[\"next_node\"] = \"build_filters\"\n",
    "        \n",
    "    print(f\"\\nNext node: {state['next_node']}\")\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_filters(state: State) -> State:\n",
    "    \"\"\"Build and refine search filters based on user needs.\"\"\"\n",
    "    \n",
    "    for interface in state[\"web_interfaces\"]:\n",
    "        filters_info = interface.get_filters_info()\n",
    "        \n",
    "        # TODO: Check if this website is useful to the user based on the filters\n",
    "        # If not continue to the next interface\n",
    "        \n",
    "        # If the website is useful, use LLM to setup the filters based on user needs\n",
    "        \n",
    "        # Define system instructions with filters information\n",
    "        system_message = SystemMessage(filters_info + \"\\n\\n\" + \"User needs:\\n\" + state[\"user_needs\"])\n",
    "\n",
    "        # Use the LLM to process the user's needs and set the filters\n",
    "        try:\n",
    "            result = llm.invoke([system_message])\n",
    "            llm_response = result.content.strip()\n",
    "\n",
    "            # Validate and set the filters for the interface\n",
    "            interface.set_filters_from_llm_response(llm_response)\n",
    "            print(f\"\\nSuccessfully set filters for: {interface.base_url}\")\n",
    "            print(f\"Updated URL: {interface.url}\")\n",
    "        except ValueError as e:\n",
    "            print(f\"Failed to set filters for {interface.base_url}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing filters for {interface.base_url}: {e}\")\n",
    "    \n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def fetch_listings_from_sources(web_interfaces: List[WebsiteInterface]) -> List[Dict[str, str]]:\n",
    "    \"\"\"Simulate retrieval of car listings from LaCentrale and mobile.de based on filters.\n",
    "    \n",
    "    Args:\n",
    "        filters (dict): Dictionary containing search filters (e.g., budget, fuel type).\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of dictionaries, each representing a car listing.\n",
    "    \"\"\"\n",
    "    listings = []\n",
    "    for interface in web_interfaces:\n",
    "        listings += await interface.crawl()\n",
    "        \n",
    "    return listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_listings(state: State) -> State:\n",
    "    \"\"\"Search for cars on LaCentrale and mobile.de based on filters.\"\"\"\n",
    "    \"\"\"Display the first listings for the user to view.\"\"\"\n",
    "    \"\"\"Synchronous wrapper for search_listings.\"\"\"\n",
    "    async def _search_listings():\n",
    "        return await fetch_listings_from_sources(state[\"web_interfaces\"])\n",
    "    \n",
    "    listings = asyncio.run(_search_listings())\n",
    "    state[\"listings\"] = listings\n",
    "    \n",
    "    print(f\"Successfully fetched {len(listings)} listings from the sources.\")\n",
    "    \n",
    "    ai_message = \"\"\n",
    "    \n",
    "    # Display the first few listings for the user to view\n",
    "    ai_message += \"Here are recent listings that match your requirements:\\n\"\n",
    "    for i, listing in enumerate(state[\"listings\"][:5], 1):\n",
    "        ai_message += f\"{i}.\\n\"\n",
    "        for key, value in listing.items():\n",
    "            formatted_key = key.replace(\"_\", \" \").capitalize()\n",
    "            ai_message += f\"   {formatted_key}: {value}\\n\"\n",
    "        ai_message += \"\\n\"  # Add an extra line for readability\n",
    "    \n",
    "    user_prompt = \"Would you like to view more details about a specific listing, or refine your search?\"\n",
    "    ai_message += user_prompt\n",
    "    \n",
    "    print(\"\\033[92m\" + ai_message + \"\\033[0m\")\n",
    "        \n",
    "    state[\"messages\"].append(AIMessage(ai_message))\n",
    "    \n",
    "    # Ask the user for input\n",
    "    state[\"messages\"].append(HumanMessage(input(user_prompt)))\n",
    "    \n",
    "    state[\"messages\"].append(SystemMessage(\"Based on the user's response, provide the next step by writing either 'select_listing LISTING_ID', 'refine_search' or 'end_conversation'.\"))\n",
    "    \n",
    "    response = llm.invoke(state[\"messages\"]).content\n",
    "    \n",
    "    state[\"messages\"].append(AIMessage(response))\n",
    "    \n",
    "    if \"select\" in response.lower():\n",
    "        state[\"next_node\"] = \"fetch_additional_info\"\n",
    "        selected_listing_id = response.split()[1].strip()\n",
    "        for i, listing in enumerate(state[\"listings\"][:5], 1):\n",
    "            if selected_listing_id in listing[\"id\"]:\n",
    "                state[\"selected_listing\"] = listing\n",
    "                break\n",
    "    elif \"refine\" in response.lower():\n",
    "        state[\"next_node\"] = \"ask_user_needs\"\n",
    "    else:\n",
    "        state[\"next_node\"] = END\n",
    "        \n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_additional_info(state: State) -> State:\n",
    "    \"\"\"Fetch more details about the selected car listing.\"\"\"\n",
    "    listing = state[\"selected_listing\"]\n",
    "    prompt = SystemMessage(\n",
    "        f\"Provide additional information about this car: {listing['title']}, \"\n",
    "        f\"including engine specifications, common issues with this model, and market value.\"\n",
    "    )\n",
    "    \n",
    "    result = llm.invoke([prompt])\n",
    "    \n",
    "    listing[\"additional_info\"] = result.content\n",
    "    \n",
    "    print(f\"\\033[92mHere is additional information about the selected car:\\n{listing['additional_info']}\\n\\033[0m\")\n",
    "    \n",
    "    # TODO: Decide either te refine the search or end the conversation\n",
    "    state[\"next_node\"] = END\n",
    "    \n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the StateGraph\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "# Define the nodes in the graph\n",
    "workflow.add_node(\"ask_user_needs\", ask_user_needs)\n",
    "workflow.add_node(\"build_filters\", build_filters)\n",
    "workflow.add_node(\"search_listings\", search_listings)\n",
    "workflow.add_node(\"fetch_additional_info\", fetch_additional_info)\n",
    "\n",
    "# Define edges\n",
    "workflow.add_conditional_edges(\"ask_user_needs\", lambda state: state[\"next_node\"], [\"build_filters\", \"ask_user_needs\"])\n",
    "workflow.add_edge(\"build_filters\", \"search_listings\")\n",
    "workflow.add_conditional_edges(\"search_listings\", lambda state: state[\"next_node\"], [\"fetch_additional_info\", \"ask_user_needs\", END])\n",
    "\n",
    "# Set the entry and exit points\n",
    "workflow.set_entry_point(\"ask_user_needs\")\n",
    "workflow.add_conditional_edges(\"fetch_additional_info\", lambda state: state[\"next_node\"], [\"ask_user_needs\", END])\n",
    "\n",
    "\n",
    "# Compile the workflow\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    Image(\n",
    "        app.get_graph().draw_mermaid_png(\n",
    "            draw_method=MermaidDrawMethod.API,\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify initial setup and function invocation\n",
    "def run_car_buyer_agent():\n",
    "    \"\"\"Run the car-buying assistant with LangGraph.\"\"\"\n",
    "        \n",
    "    messages = []\n",
    "    \n",
    "    initial_state = State(\n",
    "        user_needs={}, \n",
    "        web_interfaces=[AutotraderInterface()], \n",
    "        listings=[],\n",
    "        selected_listing={}, \n",
    "        additional_info={},\n",
    "        user_input=\"\",\n",
    "        next_node=\"\",\n",
    "        messages=messages\n",
    "    )\n",
    "    result = app.invoke(initial_state)\n",
    "    return result\n",
    "\n",
    "# Execute the agent\n",
    "car_buyer_result = run_car_buyer_agent()\n",
    "\n",
    "# Print result for debugging purposes\n",
    "print(\"Car Buyer Result:\", car_buyer_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary of the final recommendation\n",
    "if \"selected_listing\" in car_buyer_result:\n",
    "    listing = car_buyer_result[\"selected_listing\"]\n",
    "    print(f\"\\nFinal Recommendation:\\n{listing['title']} - {listing['price']} - {listing['mileage']} km\")\n",
    "    print(\"Additional Information:\")\n",
    "    for key, value in car_buyer_result[\"additional_info\"].items():\n",
    "        print(f\"{key}: {value}\")\n",
    "else:\n",
    "    print(\"No car listing selected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
